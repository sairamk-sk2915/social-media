---
title: "Social Media"
author: "Satya Shiva Sai Ram Kamma"
date: "2024-03-25"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(readxl)
library(dplyr)
library(FactoMineR)
library(factoextra)

social_media <- read_excel("social_media_cleaned.xlsx")

social_media_numeric <- select_if(social_media, is.numeric)

# Perform PCA
pca_result <- prcomp(social_media_numeric, scale = TRUE)

# Scree plot
plot(pca_result$sdev^2, type = "b", xlab = "Principal Component", ylab = "Variance Explained")


#From PCA variate representation of each PC, It’s evident that PC1 and PC2 add arround 50% of the to total variance

plot(pca_result$sdev^2, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")

# Loadings
loadings <- pca_result$rotation
print(loadings)

# Data projection onto all PCs
data_projection_all <- as.data.frame(pca_result$x)

# Matrix scatterplot for pairs of principal components
pairs(data_projection_all, col = "blue", pch = 19,
      main = "Data Visualization using All PCs")

# Visualize Eigenvalues
fviz_eig(pca_result, addlabels = TRUE)

# Visualize Variable Quality
fviz_pca_var(pca_result, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE)

# Visualize Individual Contributions
fviz_pca_ind(pca_result,
             geom.ind = "point", # Show points only
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )

# Biplot
biplot(pca_result)

# Variable correlation plot (Correlation Circle)
fviz_pca_var(pca_result, col.var = "black")

# Quality of representation of variables on dimensions 1 and 2
fviz_cos2(pca_result, choice = "var", axes = 1:2)

# Contributions of variables to principal components
fviz_contrib(pca_result, choice = "var", axes = 1, top = 10)
fviz_contrib(pca_result, choice = "var", axes = 2, top = 10)

# Visualize individual contributions
fviz_pca_ind(pca_result,
             geom.ind = "point", # Show points only
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )

library(scatterplot3d)
scatterplot3d(pca_result$x[,1:3], color = social_media$Instagram)


```


### Cluster Analysis

```{r}
library(readxl)
library(factoextra)
library(ggplot2)
library(factoextra)
library(ggfortify)
library(MASS)
library(ggrepel)
library(stats)


social_media_cluster <- read_excel("social_media_cleaned.xlsx")
data.scaled <- scale(x = social_media_cluster[, -1], center = TRUE, scale = TRUE)
data <- data.scaled
head(data)


# Perform PCA
pc <- prcomp(data.scaled)
pc_first_three <- pc$x[, 1:3]
# Perform K-means clustering on the first three principal components
set.seed(123)  # For reproducibility
k <- 3  # Number of clusters
km_clusters <- kmeans(pc_first_three, centers = k)

# Define colors for each cluster
cluster_colors <- c("red", "blue", "green")

# Plot the first three principal components with cluster assignments
plot(pc_first_three, col = cluster_colors[km_clusters$cluster], 
     main = "First Three Principal Components with Cluster Assignments", 
     xlab = "", ylab = "", pch = 20)
```
<p>It is first performs Principal Component Analysis (PCA) on scaled data, reducing its dimensionality.</p>
<p>Then, it extracts the first three principal components. Next, it applies K-means clustering to these components, dividing data into three clusters. </p>
<p>Finally, it plots the first three principal components with color-coded cluster assignments for visualization and analysis</p>

```{r}
# Take a subset of 20 rows
data_subset <- data[1:20, ]

# Perform PCA
pca_result <- prcomp(data_subset)

# Extract the first three principal components
pc_first_three <- pca_result$x[, 1:3]

# Perform hierarchical clustering on the first three principal components
hc <- hclust(dist(pc_first_three))

# Plot the dendrogram
plot(hc, main = "Dendrogram of Hierarchical Clustering (Subset of 20 Rows)",
     xlab = "Sample Index", ylab = "Distance", sub = NULL)

```

<p>The plot shows the first three principal components, performs hierarchical clustering on them, and plots a dendrogram showing the relationships between the samples based on their distances in the reduced-dimensional space.</p>

```{r}
# Visualize cluster and membership using first two Principal Components
fviz_cluster(list(data = pc$x[, 1:2], cluster = km_clusters$cluster))
```

<p>This plot visualizes clustering results by plotting data points in a two-dimensional space using the first two Principal Components. Each point is colored according to its assigned cluster, showing the grouping pattern identified by the clustering algorithm. It helps understand how data points are grouped based on their features.</p>

```{r}
# Non-hierarchical clustering (k-means)
num_clusters <- 2  
kmeans_model <- kmeans(data, centers = num_clusters)

# Membership for each cluster
table(kmeans_model$cluster)

```
<p>This represents clustering using the k-means algorithm, dividing data into two clusters. It initializes cluster centers randomly, assigning each data point to the nearest cluster. The table function counts the number of data points assigned to each cluster, providing insight into cluster membership and distribution.</p>

<b><p>This represents clustering using the k-means algorithm, dividing data into two clusters. It initializes cluster centers randomly, assigning each data point to the nearest cluster. The table function counts the number of data points assigned to each cluster, providing insight into cluster membership and distribution.</p></b>

```{r}
# Visualize cluster and membership using first two Principal Components
fviz_cluster(list(data = pc$x[, 1:2], cluster = kmeans_model$cluster))
```
<p>This plot visualizes clusters and their memberships using the first two principal components. It extracts these components from the data, then assigns each data point to a cluster using k-means clustering. Finally, it creates a visual representation showing how the data points are grouped based on their similarities in the first two principal components.</p>

<b><p>What is the relationship between the clustering results obtained through k-means algorithm and the underlying structure of the data as revealed by Principal Component Analysis (PCA)?</p></b>

```{r}
# Visualize cluster and membership using first two Principal Components for k-means
pca_result <- prcomp(data, scale = TRUE)
fviz_cluster(kmeans_model, data = pca_result$x[, 1:2], geom = "point", 
             pointsize = 2, fill = "white", main = "K-means Clustering Result (PCA)")
```
<p>This shows visualization of the clusters and their memberships using the first two Principal Components (PCs) obtained from the PCA (Principal Component Analysis) of the numerical data. First, it computes the PCA result for the numerical data and scales it. Then, it uses the fviz_cluster function to plot the clusters obtained from the k-means algorithm (kmeans_model). It represents each data point as a point on the plot, with the size set to 2 and colored white. The plot is titled “K-means Clustering Result (PCA)”. This visualization helps to understand how the data points are grouped into clusters based on their similarities, as revealed by the PCA analysis.</p>

<p>What is the relationship between the number of clusters (k) and the average silhouette width in k-means clustering, and how does this relationship inform the determination of the optimal number of clusters for a given dataset?</p>


```{r}
library(factoextra)
library(cluster)

# Calculate silhouette information for k-means clustering
sil <- silhouette(kmeans_model$cluster, dist(data))

# Visualize the silhouette plot for k-means clustering
fviz_silhouette(sil, main = "Silhouette Plot for K-means Clustering")
```

<p>This plot calculates and visualizes the silhouette information for k-means clustering. Silhouette analysis helps evaluate the quality of clustering by measuring how similar an object is to its own cluster compared to other clusters. A higher silhouette width indicates better separation of clusters, while negative values suggest that points might be assigned to the wrong clusters. This plot helps in determining the optimal number of clusters for k-means clustering and assessing the overall clustering performance.</p>

```{r}
# Create a data frame with cluster membership
data_clustered <- data.frame(data, Cluster = kmeans_model$cluster)  # Ensure conversion to data frame

# Scatter plot of data points colored by cluster membership
plot(data_clustered$Whatsapp.Wechat, data_clustered$youtube, 
     col = data_clustered$Cluster, pch = 17, 
     xlab = "Whatsapp", ylab = "Youtube",  
     main = "Scatter Plot of Clustering")
legend("topright", legend = unique(data_clustered$Cluster), 
       col = 1:max(data_clustered$Cluster), pch = 17, title = "Cluster")
```
<p>Overall, this plot visualizes clusters in the data, helping us understand how data points group together based on the Whatsapp and Youtube, with each group represented by a different color on the plot.</p>


### Factor Analysis

```{r}
library(ggplot2)
library(psych)
fa.parallel(social_media_numeric)
```
<p>Parallel analysis suggests that the number of factors = 0 and the number of components = 0</p>

```{r}
fit.pc <- principal(social_media_numeric, nfactors=2, rotate="varimax")
fit.pc
```

<p>
High absolute values (close to 1) indicate a strong relationship between the variable and the factor. #h2 explains how much variance of the variables are explained by the factors. #u2 indicates the amount of variance not explained by the factors Principal Components Analysis Call: principal(r = social_media_numeric, nfactors = 2, rotate = “varimax”) Standardized loadings (pattern matrix) based upon correlation matrix
</p>

<p>
SS loadings 2.27 1.80 Proportion Var 0.25 0.20 Cumulative Var 0.25 0.45 Proportion Explained 0.56 0.44 Cumulative Proportion 0.56 1.00</p>

<p>Mean item complexity = 1.3 Test of the hypothesis that 2 components are sufficient.</p>

<p>The root mean square of the residuals (RMSR) is 0.14 with the empirical chi square 29.01 with prob < 0.066

</p>

```{r}
round(fit.pc$values, 3)
fit.pc$loadings
fit.pc$communality
# Rotated factor scores, Notice the columns ordering: RC1, RC2
fit.pc
fit.pc$scores
fa.plot(fit.pc)

fa.diagram(fit.pc) # Visualize the relationship
vss(social_media_numeric)
```

<p>
Very Simple Structure Call: vss(x = social_media_numeric) VSS complexity 1 achieves a maximimum of 0.61 with 6 factors VSS complexity 2 achieves a maximimum of 0.78 with 7 factors

The Velicer MAP achieves a minimum of 0.06 with 1 factors BIC achieves a minimum of -53.17 with 1 factors Sample Size adjusted BIC achieves a minimum of 1.47 with 5 factors

Statistics by number of factors

</p>

```{r}
# Computing Correlation Matrix
corrm.social <- cor(social_media_numeric)
corrm.social

plot(corrm.social)
social_pca <- prcomp(social_media_numeric, scale=TRUE)
summary(social_pca)
plot(social_pca)

biplot(fit.pc)
```


<p>Overall, these techniques complement each other and can be used together to gain a comprehensive understanding of the data, uncover hidden patterns and structures, and derive meaningful insights for decision-making and further analysis</p>


#### Multiple Regression Analysis

```{r}
library(dplyr)
library(readxl)
library(car)
library(ggplot2)
library(GGally)

social_media <- read_excel("social_media_cleaned.xlsx")
social_media_numeric <- select_if(social_media, is.numeric)
```

#### Model Development
```{r}
model <- lm(social_media_numeric$Trouble_falling_asleep  ~ social_media_numeric$Instagram + social_media_numeric$LinkedIn + social_media_numeric$SnapChat + social_media_numeric$Twitter + social_media_numeric$`Whatsapp/Wechat` + social_media_numeric$youtube + social_media_numeric$OTT + social_media_numeric$Reddit,
  data = social_media_numeric
)
summary(model)
```
<p> In this step, I loaded the dataset and fitted a multiple regression model using the lm() function. The model predicts the How you felt the entire week? based on several predictor variables like: Instagram, LinkedIn, SnapChat, Twitter, Whatsapp, Youtube, OTT and Reddit acceleration.</p>

<p>The statistical significance of each coefficient, we look at their corresponding t-values and p-values. If a predictor variable has a low p-value (usually less than 0.05), it suggests that the variable is statistically significant in explaining the variability in trouble with sleep. In simpler terms, it means that the variable likely has a genuine impact on sleep issues.</p>

#### Model Acceptance

```{r}
coefficients(model)
confint(model,level=0.95)
fitted(model)
```


#### Residual Analysis
```{r}
residuals(model)
anova(model)
plot(model)

avPlots(model)

cutoff <- 17/((nrow(social_media)-length(model$coefficients)-2))
plot(model, which=4, cook.levels=cutoff)
influencePlot(model, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
qqPlot(model, main="QQ Plot")
```
```{r}
ggpairs(data=social_media_numeric, title="Social Media")

ggplot(social_media_numeric, aes(x = fitted(model), y = residuals(model))) +
  geom_point(alpha = 0.5) +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

```
<p>The residual vs. fitted plot is a tool used to evaluate the assumptions and adequacy of a regression model. It helps to identify whether the model adequately captures the underlying relationships in the data or if there are issues that need to be addressed. The plot shows a pattern of points around zero, the model is not appropriate.</p>

#### Prediction
<p>The predict() function will generate predicted values of the dependent variable (Trouble_falling_asleep) based on the provided predictors.</p>
```{r}
new_data_pd <- data.frame(
  character = "masinl",
  Instagram = 3.50,
  LinkedIn = 4.00,
  SnapChat = 1.00,
  Twitter = 5.00,
  `Whatsapp/Wechat` = 1.00,
  youtube = 2.50,
  OTT = 14.50,
  Reddit = 2.50,
  Trouble_falling_asleep = 0,
  Mood = 1,
  Productivity = 0,
  Tired_waking_up_in_morning = 3,
  `How you felt the entire week?` = 3
)

predicted_pd <- predict(model, newdata = new_data_pd)
predicted_pd
summary(predicted_pd)
```

#### Model Accuracy

```{r}
rsquared <- summary(model)$r.squared
cat("R-squared:", rsquared, "\n")
adjusted_rsquared <- summary(model)$adj.r.squared
cat("Adjusted R-squared:", adjusted_rsquared, "\n")
predictions <- predict(model)
rmse <- sqrt(mean((social_media$Instagram - predictions)^2))
cat("RMSE:", rmse, "\n")

```

### Logistic Regression Analysis
<p>To perform logistic regression analysis, we will use the glm() function.</p>

* Load all necessary packages 
* Load Data. we Used read_excel() function to read data from excel
* Now we will use glm() function to fit a logistic regression model to the data.
* Now use summary() function for logistic regression model to view coefficients, standard errors, z-values, and p-values.
* For Residual Analysis use plot() function to get Plot diagnostic plots, including residuals vs. fitted values, QQ plot of residuals, and scale-location plot, to check for homoscedasticity and normality of residuals.

#### Model Development
```{r}
library(readxl)
library(dplyr)
library(ROCR)
library(pROC)

social_media <- read_excel("social_media_cleaned.xlsx")
social_media_numeric <- select_if(social_media, is.numeric)
```

```{r}
threshold <- 200

social_media$tfs_Binary <- ifelse(social_media$Trouble_falling_asleep > threshold, 1, 0)

logit_model <- glm(tfs_Binary ~  Instagram + LinkedIn + SnapChat + Twitter + youtube + OTT + `Whatsapp/Wechat`, data = social_media, 
                    family = binomial)

```

<p>The code reads a dataset and preprocesses it to create a binary outcome variable based on a threshold.</p>
<p>It fits a logistic regression model using three predictor variables: <p>Total_Sessions, Conversion_Rate, and Avg_Session_Duration.</p>
This model development process involves specifying the model formula, fitting the model to the data, and obtaining a summary of the model's coefficients and statistical significance.</p>

#### Model Acceptance
```{r}
summary(logit_model)
anova(logit_model)
```

<p>The coefficients represent the estimated effect of each predictor variable on the log-odds of the outcome variable being in the positive class (1).</p>

<p>For example, the coefficient for Total_Sessions is approximately 0.0002231, indicating that for each unit increase in Total_Sessions, the log-odds of the outcome variable being in the positive class increases by 0.0002231 units.</p>

<p>The coefficients for Conversion_Rate and Avg_Session_Duration are 1.1609186 and -0.1110208, respectively.</p>

#### Residual Analysis
```{r}
# Residual Analysis
residuals(logit_model)
plot(logit_model)
```
<p>Function calculates the residuals for the fitted logistic regression model (logit_model). It returns a vector containing the residuals.</p>
<p>Plot() function generates diagnostic plots for the logistic regression model (logit_model).diagnostic plots including residuals vs. fitted values, quantile-quantile (Q-Q) plot, and leverage plot  </p>

#### Prediction
```{r}s

```

#### Model Accuracy
```{r}
predicted <- predict(logit_model, type = "response")
predicted_binary <- ifelse(predicted > 0.5, 1, 0)
confusion <- table(predicted_binary, social_media$tfs_Binary)
accuracy <- sum(diag(confusion)) / sum(confusion)
print(accuracy)
```
<p>The code reads a dataset from an Excel file, preprocesses it to create a binary outcome variable based on a threshold, fits a logistic regression model to predict this outcome using three predictor variables, conducts residual analysis, evaluates model performance using ROC curve and calculates AUC, makes predictions for a subset of the data, and assesses model accuracy metrics including accuracy and precision.</p>


